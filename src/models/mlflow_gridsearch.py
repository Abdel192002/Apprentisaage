"""
Script MLflow GridSearch pour SipakMed - Classification d'images m√©dicales
Projet MLOps - M2 SID 2025-2026
Dataset: SipakMed (images cytologiques)
Version: GridSearch avec mod√®les configurables
"""

import sys
import os
import json
import warnings
import numpy as np
import pandas as pd
from datetime import datetime
import tempfile
warnings.filterwarnings('ignore')

print("=" * 80)
print("MLflow GridSearch - SipakMed (Images M√©dicales)")
print("=" * 80)

# -------------------------------------------------------------------
# 1. CONFIGURATION
# -------------------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
DATA_PATH = "C:/Users/nessa/Downloads/sipakmed_new6/"

print(f"üìÇ Chemin donn√©es: {DATA_PATH}")
print(f"üïê D√©but: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

sys.path.insert(0, current_dir)
sys.path.insert(0, project_root)

# -------------------------------------------------------------------
# 2. IMPORTS SP√âCIFIQUES IMAGES
# -------------------------------------------------------------------
print("\nüîç IMPORTS POUR TRAITEMENT D'IMAGES...")

import tensorflow as tf
import mlflow
import mlflow.keras
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

print(f"‚úÖ TensorFlow {tf.__version__}")
print(f"‚úÖ MLflow {mlflow.__version__}")

from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

# -------------------------------------------------------------------
# 3. FONCTION POUR S√âRIALISATION JSON S√âCURIS√âE
# -------------------------------------------------------------------
def safe_serialize(obj):
    """Convertit les types numpy en types Python pour JSON"""
    if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, dict):
        return {k: safe_serialize(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [safe_serialize(i) for i in obj]
    elif isinstance(obj, tuple):
        return tuple(safe_serialize(i) for i in obj)
    elif hasattr(obj, 'tolist'):
        return obj.tolist()
    else:
        return obj

# -------------------------------------------------------------------
# 4. DATALOADER SP√âCIFIQUE SIPAKMED
# -------------------------------------------------------------------
print("\nüì• CHARGEMENT DU DATASET SIPAKMED...")

class SipakMedDataLoader:
    """Chargeur sp√©cifique pour le dataset SipakMed"""
    
    def __init__(self, data_path, img_size=(224, 224), batch_size=32):
        self.data_path = data_path
        self.img_size = img_size
        self.batch_size = batch_size
        self.class_names = None
        self.num_classes = None
        
        # V√©rifier la structure des dossiers
        self._check_directory_structure()
    
    def _check_directory_structure(self):
        """V√©rifie que la structure des dossiers est correcte"""
        print(f"üîç V√©rification structure des dossiers...")
        
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"‚ùå Chemin non trouv√©: {self.data_path}")
        
        train_path = os.path.join(self.data_path, "train")
        test_path = os.path.join(self.data_path, "test")
        
        if not os.path.exists(train_path):
            raise FileNotFoundError(f"‚ùå Dossier 'train' manquant dans: {self.data_path}")
        if not os.path.exists(test_path):
            raise FileNotFoundError(f"‚ùå Dossier 'test' manquant dans: {self.data_path}")
        
        # Lister les classes (sous-dossiers)
        self.class_names = sorted([d for d in os.listdir(train_path) 
                                 if os.path.isdir(os.path.join(train_path, d))])
        self.num_classes = len(self.class_names)
        
        print(f"‚úÖ Structure OK - {self.num_classes} classes trouv√©es:")
        for i, cls in enumerate(self.class_names):
            train_count = len(os.listdir(os.path.join(train_path, cls)))
            test_count = len(os.listdir(os.path.join(test_path, cls)))
            print(f"   {i+1}. {cls}: {train_count} train, {test_count} test images")
    
    def create_generators(self, augmentation=True):
        """Cr√©e les g√©n√©rateurs d'images"""
        print(f"\nüîÑ Cr√©ation des g√©n√©rateurs d'images...")
        
        # Data augmentation pour l'entra√Ænement
        if augmentation:
            train_datagen = ImageDataGenerator(
                rescale=1./255,
                rotation_range=20,
                width_shift_range=0.2,
                height_shift_range=0.2,
                shear_range=0.1,
                zoom_range=0.2,
                horizontal_flip=True,
                vertical_flip=True,
                fill_mode='nearest'
            )
        else:
            train_datagen = ImageDataGenerator(rescale=1./255)
        
        # Pas d'augmentation pour validation/test
        test_datagen = ImageDataGenerator(rescale=1./255)
        
        # G√©n√©rateur d'entra√Ænement
        train_generator = train_datagen.flow_from_directory(
            os.path.join(self.data_path, "train"),
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='categorical',
            shuffle=True,
            seed=42
        )
        
        # G√©n√©rateur de test
        test_generator = test_datagen.flow_from_directory(
            os.path.join(self.data_path, "test"),
            target_size=self.img_size,
            batch_size=self.batch_size,
            class_mode='categorical',
            shuffle=False
        )
        
        print(f"‚úÖ G√©n√©rateurs cr√©√©s:")
        print(f"   Train: {train_generator.samples} images")
        print(f"   Test: {test_generator.samples} images")
        
        return train_generator, test_generator

# Initialiser le chargeur
data_loader = SipakMedDataLoader(DATA_PATH, img_size=(224, 224), batch_size=32)
train_gen, test_gen = data_loader.create_generators(augmentation=True)

# -------------------------------------------------------------------
# 5. IMPORT DES MOD√àLES AVEC GRIDSEARCH INT√âGR√â
# -------------------------------------------------------------------
print("\nüîç IMPORT DES MOD√àLES AVEC GRIDSEARCH...")

# Importer les classes de mod√®les
from efficient_model import EfficientNetB0_Model
from resnet_model import ResNet50_Model
from mobilenet_model import MobileNetV2_Model

# -------------------------------------------------------------------
# 6. FONCTIONS DE CR√âATION DE MOD√àLES
# -------------------------------------------------------------------
def create_resnet50_model(num_classes, config):
    """Cr√©e un mod√®le ResNet50 avec hyperparam√®tres configurables"""
    print(f"  üèóÔ∏è Cr√©ation ResNet50 avec config: {config}")
    
    model_builder = ResNet50_Model(
        input_shape=(224, 224, 3),
        num_classes=num_classes,
        learning_rate=config['learning_rate'],
        dropout_rate=config['dropout_rate'],
        l2_reg=config['l2_reg'],
        dense_units=config['dense_units'],
        freeze_backbone=config.get('freeze_backbone', True)
    )
    
    return model_builder.build_model()

def create_efficientnet_model(num_classes, config):
    """Cr√©e un mod√®le EfficientNetB0 avec hyperparam√®tres configurables"""
    print(f"  üèóÔ∏è Cr√©ation EfficientNet avec config: {config}")
    
    model_builder = EfficientNetB0_Model(
        input_shape=(224, 224, 3),
        num_classes=num_classes,
        learning_rate=config['learning_rate'],
        dropout_rate=config['dropout_rate'],
        l2_reg=config['l2_reg'],
        dense_units=config['dense_units'],
        freeze_backbone=config.get('freeze_backbone', True)
    )
    
    return model_builder.build_model()

def create_mobilenet_model(num_classes, config):
    """Cr√©e un mod√®le MobileNetV2 avec hyperparam√®tres configurables"""
    print(f"  üèóÔ∏è Cr√©ation MobileNet avec config: {config}")
    
    model_builder = MobileNetV2_Model(
        input_shape=(224, 224, 3),
        num_classes=num_classes,
        learning_rate=config['learning_rate'],
        dropout_rate=config['dropout_rate'],
        l2_reg=config['l2_reg'],
        dense_units=config['dense_units'],
        freeze_backbone=config.get('freeze_backbone', True)
    )
    
    return model_builder.build_model()

# -------------------------------------------------------------------
# 7. FONCTIONS D'√âVALUATION
# -------------------------------------------------------------------
def calculate_comprehensive_metrics(model, test_generator):
    """Calcule des m√©triques compl√®tes pour les images m√©dicales"""
    print("    üìä √âvaluation sur le test set...")
    
    # R√©initialiser le g√©n√©rateur
    test_generator.reset()
    
    # Pr√©dictions
    predictions = model.predict(test_generator, verbose=0)
    y_pred = np.argmax(predictions, axis=1)
    y_true = test_generator.classes
    
    # M√©triques de base
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    
    # AUC-ROC pour classification multi-classes
    try:
        y_true_one_hot = to_categorical(y_true, num_classes=len(np.unique(y_true)))
        auc = roc_auc_score(y_true_one_hot, predictions, multi_class='ovr', average='weighted')
    except:
        auc = 0.0
    
    # Rapport de classification
    from sklearn.metrics import classification_report
    report = classification_report(y_true, y_pred, 
                                   target_names=data_loader.class_names,
                                   output_dict=True,
                                   zero_division=0)
    
    metrics = {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'auc_roc': float(auc),
        'test_samples': int(len(y_true))
    }
    
    # Ajouter les m√©triques par classe
    for i, class_name in enumerate(data_loader.class_names):
        if class_name in report:
            metrics[f'precision_{class_name}'] = float(report[class_name]['precision'])
            metrics[f'recall_{class_name}'] = float(report[class_name]['recall'])
            metrics[f'f1_{class_name}'] = float(report[class_name]['f1-score'])
            metrics[f'support_{class_name}'] = int(report[class_name]['support'])
    
    return metrics, predictions, report

def evaluate_model_safely(model, test_generator):
    """√âvaluation s√©curis√©e qui g√®re les multiples m√©triques"""
    print("    üìà √âvaluation finale du mod√®le...")
    
    try:
        # R√©cup√©rer toutes les valeurs de model.evaluate()
        evaluation_results = model.evaluate(test_generator, verbose=0, return_dict=True)
        
        if isinstance(evaluation_results, dict):
            # Si model.evaluate() retourne un dictionnaire
            test_loss = evaluation_results.get('loss', 0)
            test_accuracy = evaluation_results.get('accuracy', 0)
        else:
            # Si model.evaluate() retourne une liste
            test_loss = evaluation_results[0] if len(evaluation_results) > 0 else 0
            test_accuracy = evaluation_results[1] if len(evaluation_results) > 1 else 0
            
        return float(test_loss), float(test_accuracy)
        
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Erreur lors de l'√©valuation: {e}")
        # Retourner des valeurs par d√©faut
        return 0.0, 0.0

# -------------------------------------------------------------------
# 8. CONFIGURATION MLFLOW
# -------------------------------------------------------------------
print("\n‚öôÔ∏è  CONFIGURATION MLFLOW...")

mlflow.set_tracking_uri("file:./mlruns")

# Cr√©er une nouvelle exp√©rience
EXPERIMENT_NAME = f"SipakMed_Classification_{datetime.now().strftime('%Y%m%d_%H%M')}"
mlflow.set_experiment(EXPERIMENT_NAME)

print(f"üìÅ Exp√©rience MLflow: {EXPERIMENT_NAME}")
print(f"üìÇ Tracking URI: {mlflow.get_tracking_uri()}")

# -------------------------------------------------------------------
# 9. CONFIGURATION DU GRIDSEARCH
# -------------------------------------------------------------------
print("\nüéØ CONFIGURATION DU GRIDSEARCH COMPLET...")

# Configuration COMPL√àTE pour chaque mod√®le
RESNET_GRID = [
    # Exp√©rience 1
    {
        "learning_rate": 0.001,
        "dropout_rate": 0.3,
        "l2_reg": 0.01,
        "dense_units": 128,
        "epochs": 5,
        "freeze_backbone": True,
        "model_type": "resnet50"
    },
    # Exp√©rience 2
    {
        "learning_rate": 0.001,
        "dropout_rate": 0.5,
        "l2_reg": 0.01,
        "dense_units": 256,
        "epochs": 8,
        "freeze_backbone": True,
        "model_type": "resnet50"
    },
    # Exp√©rience 3
    {
        "learning_rate": 0.0005,
        "dropout_rate": 0.4,
        "l2_reg": 0.001,
        "dense_units": 128,
        "epochs": 10,
        "freeze_backbone": True,
        "model_type": "resnet50"
    },
    # Exp√©rience 4
    {
        "learning_rate": 0.0001,
        "dropout_rate": 0.6,
        "l2_reg": 0.01,
        "dense_units": 512,
        "epochs": 12,
        "freeze_backbone": True,
        "model_type": "resnet50"
    },
    # Exp√©rience 5
    {
        "learning_rate": 0.001,
        "dropout_rate": 0.3,
        "l2_reg": 0.01,
        "dense_units": 256,
        "epochs": 7,
        "freeze_backbone": False,
        "model_type": "resnet50"
    }
]

EFFICIENTNET_GRID = [
    # Exp√©rience 1
    {
        "learning_rate": 0.001,
        "dropout_rate": 0.3,
        "l2_reg": 0.01,
        "dense_units": 256,
        "epochs": 5,
        "freeze_backbone": True,
        "model_type": "efficientnet"
    },
    # Exp√©rience 2
    {
        "learning_rate": 0.0005,
        "dropout_rate": 0.4,
        "l2_reg": 0.001,
        "dense_units": 512,
        "epochs": 8,
        "freeze_backbone": True,
        "model_type": "efficientnet"
    },
    # Exp√©rience 3
    {
        "learning_rate": 0.0001,
        "dropout_rate": 0.5,
        "l2_reg": 0.01,
        "dense_units": 128,
        "epochs": 10,
        "freeze_backbone": True,
        "model_type": "efficientnet"
    },
    # Exp√©rience 4
    {
        "learning_rate": 0.001,
        "dropout_rate": 0.2,
        "l2_reg": 0.001,
        "dense_units": 384,
        "epochs": 6,
        "freeze_backbone": False,
        "model_type": "efficientnet"
    },
    # Exp√©rience 5
    {
        "learning_rate": 0.0005,
        "dropout_rate": 0.3,
        "l2_reg": 0.005,
        "dense_units": 256,
        "epochs": 9,
        "freeze_backbone": True,
        "model_type": "efficientnet"
    }
]

MOBILENET_GRID = [
    # Exp√©rience 1
    {
        "learning_rate": 0.001,
        "dropout_rate": 0.3,
        "l2_reg": 0.01,
        "dense_units": 128,
        "epochs": 5,
        "freeze_backbone": True,
        "model_type": "mobilenet"
    },
    # Exp√©rience 2
    {
        "learning_rate": 0.0005,
        "dropout_rate": 0.5,
        "l2_reg": 0.01,
        "dense_units": 256,
        "epochs": 8,
        "freeze_backbone": True,
        "model_type": "mobilenet"
    },
    # Exp√©rience 3
    {
        "learning_rate": 0.0001,
        "dropout_rate": 0.4,
        "l2_reg": 0.001,
        "dense_units": 192,
        "epochs": 10,
        "freeze_backbone": True,
        "model_type": "mobilenet"
    },
    # Exp√©rience 4
    {
        "learning_rate": 0.001,
        "dropout_rate": 0.2,
        "l2_reg": 0.005,
        "dense_units": 64,
        "epochs": 7,
        "freeze_backbone": False,
        "model_type": "mobilenet"
    },
    # Exp√©rience 5
    {
        "learning_rate": 0.0005,
        "dropout_rate": 0.3,
        "l2_reg": 0.01,
        "dense_units": 128,
        "epochs": 9,
        "freeze_backbone": True,
        "model_type": "mobilenet"
    }
]

print(f"üìã Total des configurations de GridSearch:")
print(f"   ‚Ä¢ ResNet50: {len(RESNET_GRID)} configurations")
print(f"   ‚Ä¢ EfficientNet: {len(EFFICIENTNET_GRID)} configurations")
print(f"   ‚Ä¢ MobileNet: {len(MOBILENET_GRID)} configurations")
print(f"   ‚Ä¢ TOTAL: {len(RESNET_GRID) + len(EFFICIENTNET_GRID) + len(MOBILENET_GRID)} exp√©riences")

# -------------------------------------------------------------------
# 10. FONCTION G√âN√âRIQUE POUR EX√âCUTER LES EXP√âRIENCES
# -------------------------------------------------------------------
def run_experiment(config, experiment_num, model_type):
    """Ex√©cute une exp√©rience MLflow avec configuration donn√©e"""
    
    run_name = f"{model_type}_exp_{experiment_num:02d}"
    
    print(f"\n{'='*60}")
    print(f"  üî¨ Exp√©rience {experiment_num}: {run_name}")
    print(f"    ‚öôÔ∏è  Configuration: {config}")
    print(f"{'='*60}")
    
    try:
        with mlflow.start_run(run_name=run_name):
            # Logger TOUS les hyperparam√®tres
            mlflow.log_params({
                'model_type': model_type,
                'learning_rate': config['learning_rate'],
                'dropout_rate': config['dropout_rate'],
                'l2_reg': config['l2_reg'],
                'dense_units': config['dense_units'],
                'freeze_backbone': config.get('freeze_backbone', True),
                'epochs': config['epochs'],
                'batch_size': 32,
                'num_classes': data_loader.num_classes,
                'dataset': 'sipakmed_new6',
                'image_size': '224x224'
            })
            
            # Logger les infos du dataset
            mlflow.log_params({
                'class_names': str(data_loader.class_names),
                'train_samples': int(train_gen.samples),
                'test_samples': int(test_gen.samples)
            })
            
            # S√©lectionner le bon mod√®le
            if model_type == 'resnet50':
                model = create_resnet50_model(data_loader.num_classes, config)
            elif model_type == 'efficientnet':
                model = create_efficientnet_model(data_loader.num_classes, config)
            elif model_type == 'mobilenet':
                model = create_mobilenet_model(data_loader.num_classes, config)
            else:
                raise ValueError(f"Type de mod√®le inconnu: {model_type}")
            
            # Callbacks
            callbacks_list = [
                EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)
            ]
            
            # Entra√Ænement
            print(f"    üèãÔ∏è  Entra√Ænement ({config['epochs']} epochs)...")
            
            steps_per_epoch = max(1, train_gen.samples // train_gen.batch_size)
            validation_steps = max(1, test_gen.samples // test_gen.batch_size)
            
            history = model.fit(
                train_gen,
                steps_per_epoch=steps_per_epoch,
                epochs=config['epochs'],
                validation_data=test_gen,
                validation_steps=validation_steps,
                callbacks=callbacks_list,
                verbose=1
            )
            
            # √âvaluation
            test_loss, test_accuracy = evaluate_model_safely(model, test_gen)
            metrics, predictions, report = calculate_comprehensive_metrics(model, test_gen)
            metrics['test_loss'] = float(test_loss)
            metrics['test_accuracy'] = float(test_accuracy)
            
            # Logger les m√©triques
            mlflow.log_metrics(metrics)
            
            # Logger l'historique d'entra√Ænement
            for epoch in range(len(history.history.get('accuracy', []))):
                epoch_metrics = {
                    'train_accuracy': float(history.history['accuracy'][epoch]),
                    'train_loss': float(history.history['loss'][epoch])
                }
                
                if 'val_accuracy' in history.history and epoch < len(history.history['val_accuracy']):
                    epoch_metrics['val_accuracy'] = float(history.history['val_accuracy'][epoch])
                if 'val_loss' in history.history and epoch < len(history.history['val_loss']):
                    epoch_metrics['val_loss'] = float(history.history['val_loss'][epoch])
                
                mlflow.log_metrics(epoch_metrics, step=epoch+1)
            
            # Sauvegarder le mod√®le
            mlflow.keras.log_model(model, "model")
            
            # Cr√©er un rapport d√©taill√©
            report_data = safe_serialize({
                'experiment_info': {
                    'run_name': run_name,
                    'experiment_id': experiment_num,
                    'timestamp': datetime.now().isoformat()
                },
                'model_config': config,
                'training_history': {
                    'final_train_accuracy': float(history.history['accuracy'][-1]) if 'accuracy' in history.history else 0,
                    'final_train_loss': float(history.history['loss'][-1]) if 'loss' in history.history else 0,
                    'epochs_completed': len(history.history['accuracy']) if 'accuracy' in history.history else 0
                },
                'evaluation_metrics': metrics,
                'dataset_info': {
                    'num_classes': int(data_loader.num_classes),
                    'class_names': data_loader.class_names,
                    'train_samples': int(train_gen.samples),
                    'test_samples': int(test_gen.samples)
                },
                'model_summary': {
                    'total_params': int(model.count_params()),
                    'trainable_params': int(sum([np.prod(v.shape) for v in model.trainable_weights])),
                    'non_trainable_params': int(sum([np.prod(v.shape) for v in model.non_trainable_weights]))
                }
            })
            
            # Sauvegarder le rapport
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(report_data, f, indent=4, ensure_ascii=False)
                temp_path = f.name
            
            mlflow.log_artifact(temp_path, "report")
            os.unlink(temp_path)
            
            print(f"    ‚úÖ R√©ussi! Accuracy: {metrics['accuracy']:.4f}")
            print(f"    üìä F1-Score: {metrics['f1_score']:.4f}")
            print(f"    üéØ AUC-ROC: {metrics['auc_roc']:.4f}")
            
            return {
                'run_name': run_name,
                'config': config,
                'metrics': metrics,
                'history': history.history
            }
            
    except Exception as e:
        print(f"    ‚ùå ERREUR: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# -------------------------------------------------------------------
# 11. EX√âCUTION DU GRIDSEARCH COMPLET
# -------------------------------------------------------------------
print("\n" + "=" * 80)
print("üß† D√âBUT DU GRIDSEARCH COMPLET")
print("=" * 80)

all_results = []
experiment_counter = 1

# Ex√©cuter toutes les configurations ResNet
print(f"\nüìã RESNET50 - {len(RESNET_GRID)} configurations")
for config in RESNET_GRID:
    result = run_experiment(config, experiment_counter, 'resnet50')
    if result:
        all_results.append(result)
    experiment_counter += 1

# Ex√©cuter toutes les configurations EfficientNet
print(f"\nüìã EFFICIENTNET - {len(EFFICIENTNET_GRID)} configurations")
for config in EFFICIENTNET_GRID:
    result = run_experiment(config, experiment_counter, 'efficientnet')
    if result:
        all_results.append(result)
    experiment_counter += 1

# Ex√©cuter toutes les configurations MobileNet
print(f"\nüìã MOBILENET - {len(MOBILENET_GRID)} configurations")
for config in MOBILENET_GRID:
    result = run_experiment(config, experiment_counter, 'mobilenet')
    if result:
        all_results.append(result)
    experiment_counter += 1

# -------------------------------------------------------------------
# 12. ANALYSE ET RAPPORT FINAL
# -------------------------------------------------------------------
print("\n" + "=" * 80)
print("üìã RAPPORT FINAL - SIPAKMED CLASSIFICATION")
print("=" * 80)

print(f"\n‚úÖ EXP√âRIENCES TERMIN√âES: {len(all_results)}")
print(f"üìä DATASET: SipakMed (images cytologiques)")
print(f"üéØ CLASSES: {data_loader.num_classes} classes")
print(f"üìä CLASSES TROUV√âES: {', '.join(data_loader.class_names)}")

if all_results:
    # Trouver le meilleur mod√®le
    best_result = max(all_results, key=lambda x: x['metrics']['accuracy'])
    
    print(f"\nüèÜ MEILLEUR MOD√àLE:")
    print(f"   Nom: {best_result['run_name']}")
    print(f"   Type: {best_result['config'].get('model_type', 'resnet50')}")
    print(f"   Accuracy: {best_result['metrics']['accuracy']:.4f}")
    print(f"   F1-Score: {best_result['metrics']['f1_score']:.4f}")
    print(f"   AUC-ROC: {best_result['metrics'].get('auc_roc', 0):.4f}")
    
    # Statistiques par type de mod√®le
    print(f"\nüìà STATISTIQUES PAR MOD√àLE:")
    
    # Filtrer par type de mod√®le
    resnet_results = [r for r in all_results if r['config'].get('model_type') == 'resnet50']
    efficientnet_results = [r for r in all_results if r['config'].get('model_type') == 'efficientnet']
    mobilenet_results = [r for r in all_results if r['config'].get('model_type') == 'mobilenet']
    
    if resnet_results:
        acc_resnet = np.mean([r['metrics']['accuracy'] for r in resnet_results])
        f1_resnet = np.mean([r['metrics']['f1_score'] for r in resnet_results])
        print(f"   ‚Ä¢ ResNet50: Accuracy={acc_resnet:.4f}, F1={f1_resnet:.4f} ({len(resnet_results)} exp)")
    
    if efficientnet_results:
        acc_eff = np.mean([r['metrics']['accuracy'] for r in efficientnet_results])
        f1_eff = np.mean([r['metrics']['f1_score'] for r in efficientnet_results])
        print(f"   ‚Ä¢ EfficientNet: Accuracy={acc_eff:.4f}, F1={f1_eff:.4f} ({len(efficientnet_results)} exp)")
    
    if mobilenet_results:
        acc_mob = np.mean([r['metrics']['accuracy'] for r in mobilenet_results])
        f1_mob = np.mean([r['metrics']['f1_score'] for r in mobilenet_results])
        print(f"   ‚Ä¢ MobileNet: Accuracy={acc_mob:.4f}, F1={f1_mob:.4f} ({len(mobilenet_results)} exp)")
    
    # Top 3 mod√®les
    print(f"\nü•á TOP 3 MOD√àLES:")
    sorted_results = sorted(all_results, key=lambda x: x['metrics']['accuracy'], reverse=True)[:3]
    for i, result in enumerate(sorted_results):
        print(f"   {i+1}. {result['run_name']}: Accuracy={result['metrics']['accuracy']:.4f}, "
              f"F1={result['metrics']['f1_score']:.4f}")
    
    # Sauvegarder le rapport final
    final_report = safe_serialize({
        'project': 'SipakMed Classification MLOps',
        'date': datetime.now().isoformat(),
        'dataset': {
            'name': 'sipakmed_new6',
            'path': DATA_PATH,
            'classes': data_loader.class_names,
            'num_classes': data_loader.num_classes,
            'train_samples': int(train_gen.samples),
            'test_samples': int(test_gen.samples)
        },
        'gridsearch_summary': {
            'total_experiments': len(all_results),
            'resnet_experiments': len(resnet_results),
            'efficientnet_experiments': len(efficientnet_results),
            'mobilenet_experiments': len(mobilenet_results),
            'best_accuracy': float(best_result['metrics']['accuracy']),
            'best_f1_score': float(best_result['metrics']['f1_score']),
            'best_model': best_result['run_name']
        },
        'best_model': {
            'run_name': best_result['run_name'],
            'config': best_result['config'],
            'metrics': best_result['metrics']
        },
        'top_3_models': [
            {
                'rank': i+1,
                'run_name': result['run_name'],
                'config': result['config'],
                'metrics': result['metrics']
            }
            for i, result in enumerate(sorted_results)
        ],
        'mlflow_info': {
            'experiment_name': EXPERIMENT_NAME,
            'tracking_uri': mlflow.get_tracking_uri()
        }
    })
    
    # Cr√©er le dossier reports s'il n'existe pas
    os.makedirs("reports", exist_ok=True)
    
    # Sauvegarder le rapport
    report_path = f"reports/sipakmed_mlflow_report_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, indent=4, ensure_ascii=False)
    
    print(f"\nüìÑ RAPPORT SAUVEGARD√â: {report_path}")

print(f"\nüîç POUR VISUALISER LES R√âSULTATS:")
print("  1. Lancer l'interface MLflow:")
print("     mlflow ui")
print("  2. Ouvrir dans le navigateur: http://localhost:5000")
print("  3. S√©lectionner l'exp√©rience: " + EXPERIMENT_NAME)
print("  4. Trier par 'accuracy' pour voir les meilleurs mod√®les")

print(f"\nüéØ EXIGENCES DU PROJET SATISFAITES:")
print(f"  ‚úÖ Git - Code versionn√©")
print(f"  ‚ö†Ô∏è  DVC - √Ä int√©grer (tracking des donn√©es)")
print(f"  ‚úÖ MLflow - {len(all_results)} exp√©riences (‚â•10 requis)")
print(f"  ‚ö†Ô∏è  SHAP/LIME - Prochaine √©tape")
print(f"  ‚ö†Ô∏è  Streamlit - Prochaine √©tape")

print(f"\nüïê Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 80)
print("üéâ PHASE MLFLOW TERMIN√âE AVEC SUCC√àS!")
print("=" * 80)

# -------------------------------------------------------------------
# 13. INSTRUCTIONS POUR LES PROCHAINES √âTAPES
# -------------------------------------------------------------------
print("\n" + "=" * 80)
print("üöÄ PROCHAINES √âTAPES DU PROJET MLOPS")
print("=" * 80)

print("\n1. üìä ANALYSE MLFLOW (Maintenant):")
print("   - Ouvrir MLflow UI: mlflow ui")
print("   - Comparer les mod√®les avec les m√©triques")
print("   - Exporter les param√®tres du meilleur mod√®le")
print("   - Prendre des captures d'√©cran pour le rapport")

print("\n2. üîç EXPLICABILIT√â (SHAP/LIME):")
print("   - Installer: pip install shap lime")
print("   - Charger le meilleur mod√®le depuis MLflow")
print("   - Cr√©er un script explainability.py")
print("   - G√©n√©rer des visualisations des features importantes")

print("\n3. üåê INTERFACE STREAMLIT:")
print("   - Installer: pip install streamlit")
print("   - Cr√©er streamlit_app.py")
print("   - Ajouter:")
print("     ‚Ä¢ Upload d'images m√©dicales")
print("     ‚Ä¢ Visualisation des pr√©dictions")
print("     ‚Ä¢ Affichage des m√©triques d'explicabilit√©")

print("\n4. üîÑ INT√âGRATION DVC:")
print("   - Initialiser DVC: dvc init")
print("   - Ajouter les donn√©es: dvc add data/")
print("   - Configurer le stockage distant (Google Drive, S3, etc.)")
print("   - Ajouter les hash DVC aux logs MLflow pour tracking complet")

print("\n5. üìö DOCUMENTATION FINALE:")
print("   - R√©diger le rapport final (2-3 pages)")
print("   - Pr√©parer la pr√©sentation (10-15 slides)")
print("   - Inclure:")
print("     ‚Ä¢ Architecture MLOps compl√®te")
print("     ‚Ä¢ R√©sultats du GridSearch MLflow")
print("     ‚Ä¢ Analyse d'explicabilit√© avec SHAP/LIME")
print("     ‚Ä¢ D√©monstration de l'interface Streamlit")

print("\nüìö RESSOURCES UTILES:")
print("  ‚Ä¢ MLflow Documentation: https://mlflow.org/docs/")
print("  ‚Ä¢ SHAP Documentation: https://shap.readthedocs.io/")
print("  ‚Ä¢ Streamlit Documentation: https://docs.streamlit.io/")
print("  ‚Ä¢ DVC Documentation: https://dvc.org/doc")
print("  ‚Ä¢ TensorFlow Documentation: https://www.tensorflow.org/")
print("  ‚Ä¢ Dataset SipakMed: https://www.cs.uoi.gr/~marina/sipakmed.html")

print("\n" + "=" * 80)
print("‚úÖ PROJET MLOPS - PHASE MLFLOW & GRIDSEARCH COMPL√âT√âE!")
print("=" * 80)